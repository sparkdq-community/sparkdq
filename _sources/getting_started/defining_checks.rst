Defining Checks
===============

With SparkDQ, data validation starts by defining **checks** that describe what ‚Äúgood data‚Äù looks like.
These checks can target either individual records or entire datasets. SparkDQ
supports both Python-native definitions and declarative configurations (such as YAML, JSON, or
database-based setups), allowing you to choose the approach that best fits your team‚Äôs workflow.

Types of Checks
---------------

SparkDQ offers two types of checks, each suited to a different level of data validation.

Row-Level Checks
^^^^^^^^^^^^^^^^

These checks validate each record individually ‚Äî for example, by detecting missing or null values,
matching patterns with regular expressions, or enforcing numeric boundaries such as minimum or
maximum values. If a row fails one of these checks, it is marked as invalid and annotated with
detailed failure context. This makes it easy to isolate problematic records for filtering,
remediation, or quarantine.

Aggregate Checks
^^^^^^^^^^^^^^^^

These checks evaluate the dataset as a whole ‚Äî or within grouped subsets ‚Äî by applying rules such
as record count constraints, uniqueness checks across one or more columns, or distributional
metrics like minimum, maximum, average, or ratio validations. If an aggregate check fails, all
rows are marked as invalid, even if they would pass individual checks. This reflects scenarios
where, for example, a dataset with too few records should cause the entire batch to be treated as suspicious.

Check Severity
--------------

Each check can be assigned a severity level to control how violations should be interpreted:

* **critical** (default): indicates a strict failure condition

* **warning**: indicates a non-blocking issue that should be recorded but not treated as invalid

The severity determines how the validation engine classifies results later on. Critical checks are typically
used to enforce quality gates, while warnings are useful for logging soft constraints or monitoring potential issues.

Python-native Configuration
---------------------------

For dynamic or code-driven use cases (e.g. notebooks, CI pipelines), you can define checks directly in Python
using type-safe config classes:

.. code-block:: python

    from sparkdq.checks import NullCheckConfig, RowCountBetweenCheckConfig
    from sparkdq.core import Severity
    from sparkdq.management import CheckSet

    check_set = CheckSet()
    check_set.add_check(
        NullCheckConfig(
            check_id="my-null-check", columns=["email"]
        )
    )
    check_set.add_check(
        RowCountBetweenCheckConfig(
            check_id="my-count-check", min_count=100, max_count=5000
        )
    )

Declarative Configuration
-------------------------

If you use a metadata-driven or config-as-code approach, SparkDQ also supports declarative check
definitions via dictionaries ‚Äî for example loaded from YAML or JSON files.

.. code-block:: yaml

    # dq_checks.yaml
    - check: null-check
      check-id: my-null-check
      columns:
        - email
      severity: warning

    - check: row-count-between-check
      check-id: my-count-check
      min_count: 100
      max_count: 5000

To load the configuration into SparkDQ, use the following code:

.. code-block:: python

    import yaml  # Optional: SparkDQ does not install pyyaml
    from sparkdq.management import CheckSet

    with open("dq_checks.yaml") as f:
        config = yaml.safe_load(f)

    check_set = CheckSet()
    check_set.add_checks_from_dicts(config)

**Note**: SparkDQ is intentionally designed to process plain Python dictionaries only ‚Äî avoiding direct
dependencies on YAML, JSON, or database connectors. This lightweight, integration-friendly design ensures
that you stay in full control of how configurations are loaded, making it easy to plug SparkDQ into any
existing system or pipeline.

Internals
---------

Both definition styles are fully compatible and can even be mixed in the same CheckSet. Internally, SparkDQ handles all checks the same way:

1. Checks are resolved via a central registry

2. Parameters are validated using Pydantic models

3. Each config is turned into a concrete check

4. Checks are executed by the validation engine

This means you can pick the approach that fits your use case ‚Äî without sacrificing flexibility or consistency.

.. raw:: html

   <hr>

üöÄ **Next Step**: Learn how to execute checks and understand results in the next section.