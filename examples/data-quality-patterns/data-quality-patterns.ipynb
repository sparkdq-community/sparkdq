{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf64644b-00ab-4c49-821f-c58ec51df18f",
   "metadata": {},
   "source": [
    "# 🚀 Fail Fast or Quarantine? How to Implement Data Quality Patterns with SparkDQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a98a987-abad-4134-aab7-29155a86d635",
   "metadata": {},
   "source": [
    "In modern data pipelines, ensuring high data quality is critical to prevent the propagation of incorrect or incomplete information. SparkDQ is a modular, Spark-native data quality framework designed specifically for PySpark.\n",
    "In this notebook, we’ll demonstrate how to implement three common data quality integration patterns using SparkDQ:\n",
    "\n",
    "* Fail Fast – stop the pipeline on any critical violation\n",
    "* Quarantine & Continue – isolate invalid rows, let the rest proceed\n",
    "* Hybrid Threshold – tolerate limited errors, abort if a threshold is exceeded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae87fa-0c97-4285-b478-92d239851022",
   "metadata": {},
   "source": [
    "## 🚖 Download NYC Yellow Taxi Dataset\n",
    "To follow this demo, we’ll use a real-world dataset from the NYC Taxi & Limousine Commission.\n",
    "This dataset contains detailed records of yellow taxi rides in January 2025 — including timestamps, distances, and fares.\n",
    "\n",
    "The following cell will create a data/ directory and download the Parquet file into it using curl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3526448-3011-4f84-93ef-d2fa909ac853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 56.4M  100 56.4M    0     0  1544k      0  0:00:37  0:00:37 --:--:-- 1306k\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data/source\n",
    "!curl -L -o data/source/yellow_tripdata_2025.parquet https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eaac2a-5f2f-4f06-b989-7189246f4693",
   "metadata": {},
   "source": [
    "## 📂 Load NYC Taxi Dataset\n",
    "Let’s load the Yellow Taxi dataset from Parquet format.\n",
    "This data includes ride timestamps, trip distances, fare amounts, and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134dd84a-4d3e-4209-9567-4319a41472d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/25 18:03:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/25 18:03:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------+-----------+---------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|passenger_count|\n",
      "+--------------------+---------------------+-------------+-----------+---------------+\n",
      "| 2025-01-01 00:18:38|  2025-01-01 00:26:59|          1.6|       10.0|              1|\n",
      "| 2025-01-01 00:32:40|  2025-01-01 00:35:13|          0.5|        5.1|              1|\n",
      "| 2025-01-01 00:44:04|  2025-01-01 00:46:01|          0.6|        5.1|              1|\n",
      "| 2025-01-01 00:14:27|  2025-01-01 00:20:01|         0.52|        7.2|              3|\n",
      "| 2025-01-01 00:21:34|  2025-01-01 00:25:06|         0.66|        5.8|              3|\n",
      "+--------------------+---------------------+-------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.parquet(\"./data/source/\")\n",
    "df = df.select(\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\",\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"passenger_count\"\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ed1c0e-f267-4f82-97b4-5dd9b0ffc37e",
   "metadata": {},
   "source": [
    "## Install SparkDQ from PyPI\n",
    "To get started, we'll install the sparkdq package directly from PyPI.\n",
    "This gives us access to all core validation features — including row- and aggregate-level checks, declarative configs, and result routing.\n",
    "\n",
    "You can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29444b77-a994-45ca-99d4-456a8f462843",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ensurepip\n",
    "!python -m pip install --quiet sparkdq pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63f24ce-74da-4ec8-be41-a74a344a3711",
   "metadata": {},
   "source": [
    "Ensure that SparkDQ is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59e2f58-e2ab-410d-8edb-cea21183285c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparkdq\n",
    "sparkdq.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9719c4c-978b-42b7-bd18-2f436b00ad41",
   "metadata": {},
   "source": [
    "## 📜 Declarative Data Quality Configuration (YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "810272cf-b2f8-49f8-b7b6-76f88b38f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheckSet:\n",
      "  - pickup-null (NullCheck)\n",
      "  - dropoff-null (NullCheck)\n",
      "  - trip-positive (NumericMinCheck)\n",
      "  - fare-positive (NumericMinCheck)\n",
      "  - chronological (ColumnLessThanCheck)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from sparkdq.management import CheckSet\n",
    "\n",
    "with open(\"dq_checks.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "check_set = CheckSet()\n",
    "check_set.add_checks_from_dicts(config)\n",
    "print(check_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08946f6-2ecc-4733-aea9-1ccc363c07d0",
   "metadata": {},
   "source": [
    "## ✅ Run Validation with SparkDQ\n",
    "Now that the checks are defined and the data is loaded, we can initialize the SparkDQ engine and run the validation.\n",
    "\n",
    "The engine will apply all configured checks to the input DataFrame and return a structured result, which can be used to filter passing and failing records, inspect errors, or calculate summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716eb6fa-61eb-4204-8b6a-983de4a962fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdq.engine import BatchDQEngine\n",
    "engine = BatchDQEngine(check_set)\n",
    "validation_result = engine.run_batch(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c09e55-f384-42fd-8ddc-94350fa185af",
   "metadata": {},
   "source": [
    "## 📊 Inspect Validation Summary\n",
    "Once the checks are applied, we can print the validation summary to get a high-level view of the data quality.\n",
    "\n",
    "In this case, the reported pass_rate is 0.94, meaning that 6% of all records failed at least one critical check.\n",
    "\n",
    "That may not sound like much — but with millions of records, it represents a significant volume of problematic data that could distort downstream metrics or lead your ML models to learn from invalid inputs.\n",
    "\n",
    "By making this visible, SparkDQ gives you the confidence to trust what moves forward — and to understand what gets filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066f6190-0e88-497a-a2fd-2f0fd49cbb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:==========================================>              (9 + 3) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary (2025-05-25 18:06:27)\n",
      "Total records:   3,475,226\n",
      "Passed records:  3,252,514\n",
      "Failed records:  222,712\n",
      "Warnings:        0\n",
      "Pass rate:       94.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "summary = validation_result.summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f469ecb-f60e-43ec-9bea-266d629a5981",
   "metadata": {},
   "source": [
    "## 🚨 Pattern #1 – Fail Fast\n",
    "\n",
    "Fail-Fast Validation is a strict and uncompromising data quality strategy. As the name suggests, the moment a critical rule is violated, the pipeline halts — no data is written, no transformation is applied, and no downstream step is triggered.\n",
    "\n",
    "This approach is ideal for environments where data correctness is non-negotiable — such as financial transactions, medical records, or regulatory reporting. In these cases, it's better to stop early than to risk propagating flawed or incomplete data through the system.\n",
    "\n",
    "### ✅ Key Principles:\n",
    "\n",
    "* Checks must be clearly defined and minimal false positives guaranteed\n",
    "* Violations are treated as hard failures\n",
    "* Developers and stakeholders get immediate feedback\n",
    "* Encourages trust and accountability across teams\n",
    "\n",
    "### Implementation in SparkDQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5b554de-28f8-42bd-ae7e-2c9074ae5fea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Critical checks failed — stopping pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m summary.all_passed:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCritical checks failed — stopping pipeline.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Critical checks failed — stopping pipeline."
     ]
    }
   ],
   "source": [
    "if not summary.all_passed:\n",
    "    raise RuntimeError(\"Critical checks failed — stopping pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386f763-34cd-4fc4-a455-1351c001ef1f",
   "metadata": {},
   "source": [
    "## 🧯 Pattern 2: Quarantine Strategy – Summary\n",
    "\n",
    "The Quarantine Strategy takes a flexible approach by allowing data pipelines to continue even when some records fail validation. Invalid rows are separated into a quarantine zone, while valid data proceeds as usual.\n",
    "\n",
    "Each quarantined record is enriched with metadata — including error types, severity levels, and timestamps — enabling teams to analyze issues, track trends, and improve data quality over time.\n",
    "\n",
    "This pattern is ideal for fast-paced environments like data lake ingestion or machine learning pipelines, where partial success is preferable to complete failure.\n",
    "\n",
    "### ✅ Key Principles of the Quarantine Strategy\n",
    "\n",
    "* Graceful Degradation: The pipeline continues to operate even when some data fails validation.\n",
    "* Data Separation: Valid and invalid records are clearly split into two distinct outputs.\n",
    "* Error Transparency: Failed records carry rich metadata (e.g., failed checks, timestamps, severity).\n",
    "* Operational Resilience: Prevents small issues from blocking critical processes.\n",
    "* Continuous Improvement: Enables teams to iteratively clean, monitor, and improve quarantined data.\n",
    "\n",
    "### Implementation in SparkDQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "974b7391-1fd5-4e57-bc70-fcbe76a63e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Write good data\n",
    "validation_result.pass_df().write.parquet(\"data/trusted-zone/\")\n",
    "\n",
    "# Write bad data\n",
    "validation_result.fail_df().write.parquet(\"data//quarantine-zone/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac68b0c-496b-46f3-b411-91c9636d0ba3",
   "metadata": {},
   "source": [
    "## ⚖️ Pattern 3: Hybrid Strategy – Quality Threshold\n",
    "The Hybrid Strategy, also known as the Quality Threshold approach, combines the strengths of both the Fail-Fast and Quarantine patterns. Instead of aborting immediately or accepting all records, it allows a limited amount of bad data, but enforces a strict upper threshold for how much is tolerable.\n",
    "\n",
    "This pattern is useful in scenarios where small data issues are acceptable, but major violations must still trigger a failure. It offers a balanced trade-off between reliability and flexibility.\n",
    "\n",
    "### ✅ Key Principles of the Hybrid Strategy\n",
    "\n",
    "* Controlled Tolerance: Defines an acceptable error ratio (e.g., 20%) that should not be exceeded.\n",
    "* Automated Decision Logic: Validation results are programmatically evaluated post-check.\n",
    "* Conditional Continuation: The pipeline proceeds only if the error ratio is within safe limits.\n",
    "* Balanced Risk Management: Avoids full pipeline stops due to a few bad records, while still protecting against large-scale issues.\n",
    "* Customizable Thresholds: Different pipelines or datasets can define their own risk appetite.\n",
    "\n",
    "### Implementation in SparkDQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f03ee9d2-2830-4a8e-8216-cd605f255d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "if summary.pass_rate < 0.8:\n",
    "    raise RuntimeError(\"Pass ratio is too low.\")\n",
    "\n",
    "validation_result.pass_df().write.parquet(\"data/trusted-zone/\", mode=\"overwrite\")\n",
    "validation_result.fail_df().write.parquet(\"data//quarantine-zone/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb90240-8d7d-49ef-9a95-cddddb06e872",
   "metadata": {},
   "source": [
    "## 🔁 Conclusion\n",
    "Data quality is not a one-size-fits-all discipline — it requires thoughtful strategies tailored to the context, data maturity, and business criticality of each pipeline.\n",
    "\n",
    "With SparkDQ, you can implement a wide range of validation patterns natively in PySpark:\n",
    "\n",
    "* Use Fail-Fast when correctness is paramount and every violation is a blocker.\n",
    "* Apply the Quarantine Strategy to build resilient systems that isolate and enrich invalid records without halting execution.\n",
    "* Leverage the Hybrid Quality Threshold to balance flexibility and control — accepting some bad data, but never too much.\n",
    "\n",
    "These patterns are not mutually exclusive. A single pipeline might use all three at different stages, depending on the purpose and reliability requirements of each component.\n",
    "\n",
    "Ultimately, SparkDQ empowers data teams to move from ad-hoc data checks to structured, testable, and observable validation logic — enabling safer pipelines, faster debugging, and better collaboration between producers and consumers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
