{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23c36ac",
   "metadata": {},
   "source": [
    "# 🚖 From Dirty to Trusted: NYC Taxi Data Validation with SparkDQ\n",
    "\n",
    "In this notebook, we explore how to turn messy, real-world data into trustworthy input for machine learning — using [**SparkDQ**](https://github.com/sparkdq-community/sparkdq), a modern, extensible data quality framework built for PySpark.\n",
    "\n",
    "We’ll validate a public dataset from the NYC Taxi & Limousine Commission, clean it with declarative quality rules, and demonstrate how unchecked records can distort ML models.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to apply rule-based validation to large Spark DataFrames\n",
    "- How to isolate valid vs. invalid data records\n",
    "- How bad data affects model performance\n",
    "- How SparkDQ gives you full control over the quality in your data pipeline\n",
    "\n",
    "Let's dive in. 🧪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a7aa5-e634-44b0-a77d-3f3254b4f816",
   "metadata": {},
   "source": [
    "## Download NYC Yellow Taxi Dataset\n",
    "\n",
    "To follow this demo, we’ll use a real-world dataset from the NYC Taxi & Limousine Commission.  \n",
    "This dataset contains detailed records of yellow taxi rides in January 2025 — including timestamps, distances, and fares.\n",
    "\n",
    "The following cell will create a `data/` directory and download the Parquet file into it using `curl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda8668f-5f6e-40df-a955-446d271ce390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 56.4M  100 56.4M    0     0  1342k      0  0:00:43  0:00:43 --:--:--  761k\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!curl -L -o data/yellow_tripdata_2025.parquet https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a6785-2266-4a96-ac53-15f592d1b58a",
   "metadata": {},
   "source": [
    "## The Messiness of Real-World Taxi Data\n",
    "\n",
    "The NYC Yellow Taxi dataset is massive and publicly available — but like most real-world data, it’s far from clean.\n",
    "\n",
    "Here are some of the common issues you’ll encounter:\n",
    "\n",
    "- ❌ Missing values in `pickup_datetime` or `dropoff_datetime`  \n",
    "- ❌ Negative or zero `trip_distance`  \n",
    "- ❌ Negative or zero `fare_amount`  \n",
    "- ⏱️ Timestamps that don’t make sense (e.g., dropoff before pickup)  \n",
    "- ⚠️ Unexpected data types or nulls in `passenger_count`\n",
    "\n",
    "These aren’t just minor problems — they can **break downstream logic**, **distort features**, or even lead your ML model to learn from garbage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad77004-c13d-4d08-8a62-bb816bedbda7",
   "metadata": {},
   "source": [
    "## 📂 Load NYC Taxi Dataset\n",
    "\n",
    "We now load the Parquet-formatted Yellow Taxi dataset for January 2023.  \n",
    "This data includes timestamps, trip distance, fare amounts, and other ride metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "413916e8-ac05-4dc9-91ff-9b5653d7a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/25 18:40:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------+-----------+---------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|passenger_count|\n",
      "+--------------------+---------------------+-------------+-----------+---------------+\n",
      "| 2025-01-01 00:18:38|  2025-01-01 00:26:59|          1.6|       10.0|              1|\n",
      "| 2025-01-01 00:32:40|  2025-01-01 00:35:13|          0.5|        5.1|              1|\n",
      "| 2025-01-01 00:44:04|  2025-01-01 00:46:01|          0.6|        5.1|              1|\n",
      "| 2025-01-01 00:14:27|  2025-01-01 00:20:01|         0.52|        7.2|              3|\n",
      "| 2025-01-01 00:21:34|  2025-01-01 00:25:06|         0.66|        5.8|              3|\n",
      "+--------------------+---------------------+-------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.parquet(\"./data/\")\n",
    "df = df.select(\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\",\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"passenger_count\"\n",
    ")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54e0b0-1896-436f-94c7-c717e462ca8c",
   "metadata": {},
   "source": [
    "## Install SparkDQ from PyPI\n",
    "\n",
    "To get started, we'll install the `sparkdq` package directly from [PyPI](https://pypi.org/project/sparkdq/).  \n",
    "This gives us access to all core validation features — including row- and aggregate-level checks, declarative configs, and result routing.\n",
    "\n",
    "You can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a108fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m ensurepip\n",
    "!python -m pip install --quiet sparkdq pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a790e-ea70-4750-90f6-16202c8f42d1",
   "metadata": {},
   "source": [
    "Ensure that ``SparkDQ`` is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44896975-d855-44dc-9fdb-7bff3ea6b5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparkdq\n",
    "sparkdq.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fbb579-d943-4f20-8511-95b0efe09b99",
   "metadata": {},
   "source": [
    "## 📜 Declarative Data Quality Configuration (YAML)\n",
    "\n",
    "Instead of defining checks directly in Python, SparkDQ also supports loading them from external configuration files.  \n",
    "This allows for better separation of logic and makes it easy to manage or reuse validations across different pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1536309a-f220-4cd7-81d2-005265d940f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheckSet:\n",
      "  - pickup-null (NullCheck)\n",
      "  - dropoff-null (NullCheck)\n",
      "  - trip-positive (NumericMinCheck)\n",
      "  - fare-positive (NumericMinCheck)\n",
      "  - chronological (ColumnLessThanCheck)\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from sparkdq.management import CheckSet\n",
    "\n",
    "with open(\"dq_checks.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "check_set = CheckSet()\n",
    "check_set.add_checks_from_dicts(config)\n",
    "print(check_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4df7e-6049-4e9b-a1fc-7c6c539a7958",
   "metadata": {},
   "source": [
    "## ✅ Run Validation with SparkDQ\n",
    "\n",
    "Now that the checks are defined and the data is loaded, we can initialize the SparkDQ engine and run the validation.\n",
    "\n",
    "The engine will apply all configured checks to the input DataFrame and return a structured result, which can be used to filter passing and failing records, inspect errors, or calculate summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a254617a-1c29-420b-8afc-39dec66d057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdq.engine import BatchDQEngine\n",
    "engine = BatchDQEngine(check_set)\n",
    "validation_result = engine.run_batch(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64785f0c-6721-4562-be5f-bd1f9d11c0ea",
   "metadata": {},
   "source": [
    "## 📊 Inspect Validation Summary\n",
    "\n",
    "Once the checks are applied, we can print the validation summary to get a high-level view of the data quality.\n",
    "\n",
    "In this case, the reported `pass_rate` is **0.94**, meaning that **6% of all records failed at least one critical check**.\n",
    "\n",
    "That may not sound like much — but with millions of records, it represents a significant volume of problematic data that could distort downstream metrics or lead your ML models to learn from invalid inputs.\n",
    "\n",
    "By making this visible, SparkDQ gives you the confidence to trust what moves forward — and to understand what gets filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f37f9c-5925-45da-8a25-bdebb81ec40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===========================================>              (9 + 3) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Summary (2025-05-25 18:40:34)\n",
      "Total records:   3,475,226\n",
      "Passed records:  3,252,514\n",
      "Failed records:  222,712\n",
      "Warnings:        0\n",
      "Pass rate:       94.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(validation_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857012fe-fc23-4cf4-a39c-93b85756ffdb",
   "metadata": {},
   "source": [
    "## ❌ Inspect Failed Records and Their Validation Errors\n",
    "\n",
    "Let’s now take a closer look at the rows that didn’t pass validation.\n",
    "\n",
    "Each failed record includes a special `_dq_errors` column, which lists all the checks that were violated for that row.  \n",
    "This makes it easy to understand **why a record was rejected** and which rules were responsible.\n",
    "\n",
    "By inspecting these errors, we can spot recurring data quality issues, debug upstream problems, or even build monitoring around specific check failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89ba9911-1798-4e30-b3c7-dd332c5059c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------------------------------------------------------------------------------------------+\n",
      "|trip_distance|fare_amount|_dq_errors                                                                                  |\n",
      "+-------------+-----------+--------------------------------------------------------------------------------------------+\n",
      "|0.71         |-7.2       |[{NumericMinCheck, fare-positive, critical}]                                                |\n",
      "|0.69         |-6.5       |[{NumericMinCheck, fare-positive, critical}]                                                |\n",
      "|0.0          |20.06      |[{NumericMinCheck, trip-positive, critical}, {ColumnLessThanCheck, chronological, critical}]|\n",
      "|0.97         |-16.3      |[{NumericMinCheck, fare-positive, critical}]                                                |\n",
      "|1.42         |-12.1      |[{NumericMinCheck, fare-positive, critical}]                                                |\n",
      "+-------------+-----------+--------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bad_data = validation_result.fail_df()\n",
    "bad_data.select(\"trip_distance\", \"fare_amount\", \"_dq_errors\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06668f85-d239-449c-970c-dc4f4ce69945",
   "metadata": {},
   "source": [
    "This feedback loop is valuable not only for debugging, but also for improving upstream data collection or building alerts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d0fe7-a7de-4c80-bd33-259f0228481e",
   "metadata": {},
   "source": [
    "## ✅ Extract Passing Records Only\n",
    "\n",
    "After validation, SparkDQ allows you to easily extract only the records that passed all critical checks.  \n",
    "This lets you continue your data pipeline with full confidence that downstream steps — such as transformations, reporting, or machine learning — are based on **clean and trusted data**.\n",
    "\n",
    "With just one line, you can access the passing subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c36d805-554a-4106-bb80-66f407ceca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------+-----------+---------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|passenger_count|\n",
      "+--------------------+---------------------+-------------+-----------+---------------+\n",
      "|2025-01-01 00:18:38 |2025-01-01 00:26:59  |1.6          |10.0       |1              |\n",
      "|2025-01-01 00:32:40 |2025-01-01 00:35:13  |0.5          |5.1        |1              |\n",
      "|2025-01-01 00:44:04 |2025-01-01 00:46:01  |0.6          |5.1        |1              |\n",
      "|2025-01-01 00:14:27 |2025-01-01 00:20:01  |0.52         |7.2        |3              |\n",
      "|2025-01-01 00:21:34 |2025-01-01 00:25:06  |0.66         |5.8        |3              |\n",
      "+--------------------+---------------------+-------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "good_data = validation_result.pass_df()\n",
    "good_data.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ab287-358a-46dd-985a-823533e143ca",
   "metadata": {},
   "source": [
    "## 🧾 Conclusion\n",
    "\n",
    "In this demo, we explored how even a simple real-world dataset — like NYC Yellow Taxi rides — can benefit from structured, automated validation using **SparkDQ**.\n",
    "\n",
    "We saw that:\n",
    "\n",
    "- ✅ Checks can be defined declaratively via clean YAML configurations  \n",
    "- 🔍 Passing records can be easily extracted for safe downstream processing  \n",
    "- ❌ Failing records are clearly annotated and can be further analyzed or corrected\n",
    "\n",
    "With SparkDQ, you gain full control over data quality enforcement:  \n",
    "You can route only validated records into your machine learning pipeline — ensuring that **models are trained only on trustworthy data**.\n",
    "\n",
    "Invalid records don't vanish silently. Instead, they become visible, traceable, and improvable — which allows you to build smarter, more resilient data workflows.\n",
    "\n",
    "Whether you're validating input data, cleaning for analytics, or building ML features:  \n",
    "**SparkDQ helps make quality a first-class citizen in every PySpark pipeline.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
